{"ast":null,"code":"import * as dagPB from '@ipld/dag-pb';\nimport { Bucket, createHAMT } from 'hamt-sharding';\nimport { DirSharded } from './dir-sharded.js';\nimport { logger } from '@libp2p/logger';\nimport { UnixFS } from 'ipfs-unixfs';\nimport last from 'it-last';\nimport { CID } from 'multiformats/cid';\nimport { hamtHashCode, hamtHashFn, hamtBucketBits } from './hamt-constants.js';\nconst log = logger('ipfs:mfs:core:utils:hamt-utils');\n/**\n * @typedef {import('multiformats/cid').Version} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\n\nexport const updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data');\n  } // update parent with new bit field\n\n\n  const data = Uint8Array.from(bucket._children.bitField().reverse());\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  });\n  const hasher = await context.hashers.getHasher(options.hashAlg);\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  };\n  const buf = dagPB.encode(parent);\n  const hash = await hasher.digest(buf);\n  const cid = CID.create(options.cidVersion, dagPB.code, hash);\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf);\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  };\n};\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\n\nexport const recreateHamtLevel = async (context, links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent);\n\n  parentBucket._putObjectAt(positionAtParent, bucket);\n\n  await addLinksToHamtBucket(context, links, bucket, rootBucket);\n  return bucket;\n};\n/**\n * @param {PBLink[]} links\n */\n\nexport const recreateInitialHamtLevel = async links => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  }); // populate sub bucket but do not recurse as we do not want to pull whole shard in\n\n  await Promise.all(links.map(async link => {\n    const linkName = link.Name || '';\n\n    if (linkName.length === 2) {\n      const pos = parseInt(linkName, 16);\n      const subBucket = new Bucket({\n        hash: bucket._options.hash,\n        bits: bucket._options.bits\n      }, bucket, pos);\n\n      bucket._putObjectAt(pos, subBucket);\n\n      return Promise.resolve();\n    }\n\n    return bucket.put(linkName.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n  return bucket;\n};\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\n\nexport const addLinksToHamtBucket = async (context, links, bucket, rootBucket) => {\n  await Promise.all(links.map(async link => {\n    const linkName = link.Name || '';\n\n    if (linkName.length === 2) {\n      log('Populating sub bucket', linkName);\n      const pos = parseInt(linkName, 16);\n      const block = await context.repo.blocks.get(link.Hash);\n      const node = dagPB.decode(block);\n      const subBucket = new Bucket({\n        hash: rootBucket._options.hash,\n        bits: rootBucket._options.bits\n      }, bucket, pos);\n\n      bucket._putObjectAt(pos, subBucket);\n\n      await addLinksToHamtBucket(context, node.Links, subBucket, rootBucket);\n      return Promise.resolve();\n    }\n\n    return rootBucket.put(linkName.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n};\n/**\n * @param {number} position\n */\n\nexport const toPrefix = position => {\n  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);\n};\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\n\nexport const generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links);\n  const position = await rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    }); // @ts-expect-error - only the root bucket's parent will be undefined\n\n    currentBucket = currentBucket._parent;\n  }\n\n  path.reverse();\n  path[0].node = rootNode; // load PbNode for each path segment\n\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i];\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path');\n    } // find prefix in links\n\n\n    const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n      continue;\n    } // found entry\n\n\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n      // return path\n\n      continue;\n    } // found subshard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const block = await context.repo.blocks.get(link.Hash);\n    const node = dagPB.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(context, node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName); // i--\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n\n    const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n    await addLinksToHamtBucket(context, node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\nexport const createShard = async function (context, contents) {\n  let options = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options);\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n\n  const res = await last(shard.flush(context.repo.blocks));\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result');\n  }\n\n  return res;\n};","map":{"version":3,"sources":["C:/Users/Akshay Mishra/OneDrive/Desktop/twitter-clone-dapp/node_modules/ipfs-core/src/components/files/utils/hamt-utils.js"],"names":["dagPB","Bucket","createHAMT","DirSharded","logger","UnixFS","last","CID","hamtHashCode","hamtHashFn","hamtBucketBits","log","updateHamtDirectory","context","links","bucket","options","parent","Data","Error","data","Uint8Array","from","_children","bitField","reverse","node","unmarshal","dir","type","fanout","tableSize","hashType","mode","mtime","hasher","hashers","getHasher","hashAlg","marshal","Links","sort","a","b","Name","localeCompare","buf","encode","hash","digest","cid","create","cidVersion","code","flush","repo","blocks","put","size","reduce","sum","link","Tsize","length","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","_options","bits","_putObjectAt","addLinksToHamtBucket","recreateInitialHamtLevel","hashFn","Promise","all","map","linkName","pos","parseInt","subBucket","resolve","substring","Hash","block","get","decode","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","nextSegment","createShard","contents","shard","root","undefined","parentKey","dirty","flat","_bucket","name","res"],"mappings":"AAAA,OAAO,KAAKA,KAAZ,MAAuB,cAAvB;AACA,SACEC,MADF,EAEEC,UAFF,QAGO,eAHP;AAIA,SAASC,UAAT,QAA2B,kBAA3B;AACA,SAASC,MAAT,QAAuB,gBAAvB;AACA,SAASC,MAAT,QAAuB,aAAvB;AACA,OAAOC,IAAP,MAAiB,SAAjB;AACA,SAASC,GAAT,QAAoB,kBAApB;AACA,SACEC,YADF,EAEEC,UAFF,EAGEC,cAHF,QAIO,qBAJP;AAMA,MAAMC,GAAG,GAAGP,MAAM,CAAC,gCAAD,CAAlB;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMQ,mBAAmB,GAAG,OAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B,KAA2C;AAC5E,MAAI,CAACA,OAAO,CAACC,MAAR,CAAeC,IAApB,EAA0B;AACxB,UAAM,IAAIC,KAAJ,CAAU,4DAAV,CAAN;AACD,GAH2E,CAK5E;;;AACA,QAAMC,IAAI,GAAGC,UAAU,CAACC,IAAX,CAAgBP,MAAM,CAACQ,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAhB,CAAb;AACA,QAAMC,IAAI,GAAGrB,MAAM,CAACsB,SAAP,CAAiBX,OAAO,CAACC,MAAR,CAAeC,IAAhC,CAAb;AACA,QAAMU,GAAG,GAAG,IAAIvB,MAAJ,CAAW;AACrBwB,IAAAA,IAAI,EAAE,wBADe;AAErBT,IAAAA,IAFqB;AAGrBU,IAAAA,MAAM,EAAEf,MAAM,CAACgB,SAAP,EAHa;AAIrBC,IAAAA,QAAQ,EAAExB,YAJW;AAKrByB,IAAAA,IAAI,EAAEP,IAAI,CAACO,IALU;AAMrBC,IAAAA,KAAK,EAAER,IAAI,CAACQ;AANS,GAAX,CAAZ;AASA,QAAMC,MAAM,GAAG,MAAMtB,OAAO,CAACuB,OAAR,CAAgBC,SAAhB,CAA0BrB,OAAO,CAACsB,OAAlC,CAArB;AACA,QAAMrB,MAAM,GAAG;AACbC,IAAAA,IAAI,EAAEU,GAAG,CAACW,OAAJ,EADO;AAEbC,IAAAA,KAAK,EAAE1B,KAAK,CAAC2B,IAAN,CAAW,CAACC,CAAD,EAAIC,CAAJ,KAAU,CAACD,CAAC,CAACE,IAAF,IAAU,EAAX,EAAeC,aAAf,CAA6BF,CAAC,CAACC,IAAF,IAAU,EAAvC,CAArB;AAFM,GAAf;AAIA,QAAME,GAAG,GAAG9C,KAAK,CAAC+C,MAAN,CAAa9B,MAAb,CAAZ;AACA,QAAM+B,IAAI,GAAG,MAAMb,MAAM,CAACc,MAAP,CAAcH,GAAd,CAAnB;AACA,QAAMI,GAAG,GAAG3C,GAAG,CAAC4C,MAAJ,CAAWnC,OAAO,CAACoC,UAAnB,EAA+BpD,KAAK,CAACqD,IAArC,EAA2CL,IAA3C,CAAZ;;AAEA,MAAIhC,OAAO,CAACsC,KAAZ,EAAmB;AACjB,UAAMzC,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBP,GAAxB,EAA6BJ,GAA7B,CAAN;AACD;;AAED,SAAO;AACLpB,IAAAA,IAAI,EAAET,MADD;AAELiC,IAAAA,GAFK;AAGLQ,IAAAA,IAAI,EAAE5C,KAAK,CAAC6C,MAAN,CAAa,CAACC,GAAD,EAAMC,IAAN,KAAeD,GAAG,IAAIC,IAAI,CAACC,KAAL,IAAc,CAAlB,CAA/B,EAAqDhB,GAAG,CAACiB,MAAzD;AAHD,GAAP;AAKD,CAnCM;AAqCP;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMC,iBAAiB,GAAG,OAAOnD,OAAP,EAAgBC,KAAhB,EAAuBmD,UAAvB,EAAmCC,YAAnC,EAAiDC,gBAAjD,KAAsE;AACrG;AACA,QAAMpD,MAAM,GAAG,IAAId,MAAJ,CAAW;AACxB+C,IAAAA,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADF;AAExBqB,IAAAA,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;AAFF,GAAX,EAGZH,YAHY,EAGEC,gBAHF,CAAf;;AAIAD,EAAAA,YAAY,CAACI,YAAb,CAA0BH,gBAA1B,EAA4CpD,MAA5C;;AAEA,QAAMwD,oBAAoB,CAAC1D,OAAD,EAAUC,KAAV,EAAiBC,MAAjB,EAAyBkD,UAAzB,CAA1B;AAEA,SAAOlD,MAAP;AACD,CAXM;AAaP;AACA;AACA;;AACA,OAAO,MAAMyD,wBAAwB,GAAG,MAAO1D,KAAP,IAAiB;AACvD,QAAMC,MAAM,GAAGb,UAAU,CAAC;AACxBuE,IAAAA,MAAM,EAAEhE,UADgB;AAExB4D,IAAAA,IAAI,EAAE3D;AAFkB,GAAD,CAAzB,CADuD,CAMvD;;AACA,QAAMgE,OAAO,CAACC,GAAR,CACJ7D,KAAK,CAAC8D,GAAN,CAAU,MAAMf,IAAN,IAAc;AACtB,UAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAL,IAAa,EAA/B;;AAEA,QAAIiC,QAAQ,CAACd,MAAT,KAAoB,CAAxB,EAA2B;AACzB,YAAMe,GAAG,GAAGC,QAAQ,CAACF,QAAD,EAAW,EAAX,CAApB;AAEA,YAAMG,SAAS,GAAG,IAAI/E,MAAJ,CAAW;AAC3B+C,QAAAA,IAAI,EAAEjC,MAAM,CAACqD,QAAP,CAAgBpB,IADK;AAE3BqB,QAAAA,IAAI,EAAEtD,MAAM,CAACqD,QAAP,CAAgBC;AAFK,OAAX,EAGftD,MAHe,EAGP+D,GAHO,CAAlB;;AAIA/D,MAAAA,MAAM,CAACuD,YAAP,CAAoBQ,GAApB,EAAyBE,SAAzB;;AAEA,aAAON,OAAO,CAACO,OAAR,EAAP;AACD;;AAED,WAAOlE,MAAM,CAAC0C,GAAP,CAAWoB,QAAQ,CAACK,SAAT,CAAmB,CAAnB,CAAX,EAAkC;AACvCxB,MAAAA,IAAI,EAAEG,IAAI,CAACC,KAD4B;AAEvCZ,MAAAA,GAAG,EAAEW,IAAI,CAACsB;AAF6B,KAAlC,CAAP;AAID,GAnBD,CADI,CAAN;AAuBA,SAAOpE,MAAP;AACD,CA/BM;AAiCP;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMwD,oBAAoB,GAAG,OAAO1D,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BkD,UAA/B,KAA8C;AAChF,QAAMS,OAAO,CAACC,GAAR,CACJ7D,KAAK,CAAC8D,GAAN,CAAU,MAAMf,IAAN,IAAc;AACtB,UAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAL,IAAa,EAA/B;;AAEA,QAAIiC,QAAQ,CAACd,MAAT,KAAoB,CAAxB,EAA2B;AACzBpD,MAAAA,GAAG,CAAC,uBAAD,EAA0BkE,QAA1B,CAAH;AACA,YAAMC,GAAG,GAAGC,QAAQ,CAACF,QAAD,EAAW,EAAX,CAApB;AACA,YAAMO,KAAK,GAAG,MAAMvE,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoB6B,GAApB,CAAwBxB,IAAI,CAACsB,IAA7B,CAApB;AACA,YAAMzD,IAAI,GAAG1B,KAAK,CAACsF,MAAN,CAAaF,KAAb,CAAb;AAEA,YAAMJ,SAAS,GAAG,IAAI/E,MAAJ,CAAW;AAC3B+C,QAAAA,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADC;AAE3BqB,QAAAA,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;AAFC,OAAX,EAGftD,MAHe,EAGP+D,GAHO,CAAlB;;AAIA/D,MAAAA,MAAM,CAACuD,YAAP,CAAoBQ,GAApB,EAAyBE,SAAzB;;AAEA,YAAMT,oBAAoB,CAAC1D,OAAD,EAAUa,IAAI,CAACc,KAAf,EAAsBwC,SAAtB,EAAiCf,UAAjC,CAA1B;AAEA,aAAOS,OAAO,CAACO,OAAR,EAAP;AACD;;AAED,WAAOhB,UAAU,CAACR,GAAX,CAAeoB,QAAQ,CAACK,SAAT,CAAmB,CAAnB,CAAf,EAAsC;AAC3CxB,MAAAA,IAAI,EAAEG,IAAI,CAACC,KADgC;AAE3CZ,MAAAA,GAAG,EAAEW,IAAI,CAACsB;AAFiC,KAAtC,CAAP;AAID,GAxBD,CADI,CAAN;AA2BD,CA5BM;AA8BP;AACA;AACA;;AACA,OAAO,MAAMI,QAAQ,GAAIC,QAAD,IAAc;AACpC,SAAOA,QAAQ,CACZC,QADI,CACK,EADL,EAEJC,WAFI,GAGJC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJT,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CANM;AAQP;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMU,YAAY,GAAG,OAAO/E,OAAP,EAAgBgF,QAAhB,EAA0BC,QAA1B,KAAuC;AACjE;AACA,QAAM7B,UAAU,GAAG,MAAMO,wBAAwB,CAACsB,QAAQ,CAACtD,KAAV,CAAjD;AACA,QAAMgD,QAAQ,GAAG,MAAMvB,UAAU,CAAC8B,oBAAX,CAAgCF,QAAhC,CAAvB,CAHiE,CAKjE;;AACA;;AACA,QAAMG,IAAI,GAAG,CAAC;AACZjF,IAAAA,MAAM,EAAEyE,QAAQ,CAACzE,MADL;AAEZkF,IAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACV,GAAV;AAFJ,GAAD,CAAb;AAIA,MAAIoB,aAAa,GAAGV,QAAQ,CAACzE,MAA7B;;AAEA,SAAOmF,aAAa,KAAKjC,UAAzB,EAAqC;AACnC+B,IAAAA,IAAI,CAACG,IAAL,CAAU;AACRpF,MAAAA,MAAM,EAAEmF,aADA;AAERD,MAAAA,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAf;AAFR,KAAV,EADmC,CAMnC;;AACAF,IAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,EAAAA,IAAI,CAACvE,OAAL;AACAuE,EAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQtE,IAAR,GAAeoE,QAAf,CAxBiE,CA0BjE;;AACA,OAAK,IAAIQ,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAACjC,MAAzB,EAAiCuC,CAAC,EAAlC,EAAsC;AACpC,UAAMC,OAAO,GAAGP,IAAI,CAACM,CAAD,CAApB;;AAEA,QAAI,CAACC,OAAO,CAAC7E,IAAb,EAAmB;AACjB,YAAM,IAAIP,KAAJ,CAAU,8BAAV,CAAN;AACD,KALmC,CAOpC;;;AACA,UAAM0C,IAAI,GAAG0C,OAAO,CAAC7E,IAAR,CAAac,KAAb,CACVgE,MADU,CACH3C,IAAI,IAAI,CAACA,IAAI,CAACjB,IAAL,IAAa,EAAd,EAAkBsC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCqB,OAAO,CAACN,MADnD,EAEVQ,GAFU,EAAb,CARoC,CAYpC;;AACA,QAAI,CAAC5C,IAAL,EAAW;AACT;AACAlD,MAAAA,GAAG,CAAE,QAAO4F,OAAO,CAACN,MAAO,GAAEJ,QAAS,gBAAnC,CAAH,CAFS,CAGT;;AACA;AACD,KAlBmC,CAoBpC;;;AACA,QAAIhC,IAAI,CAACjB,IAAL,KAAe,GAAE2D,OAAO,CAACN,MAAO,GAAEJ,QAAS,EAA/C,EAAkD;AAChDlF,MAAAA,GAAG,CAAE,QAAO4F,OAAO,CAACN,MAAO,GAAEJ,QAAS,mBAAnC,CAAH,CADgD,CAEhD;AACA;;AACA;AACD,KA1BmC,CA4BpC;;;AACAlF,IAAAA,GAAG,CAAE,kBAAiB4F,OAAO,CAACN,MAAO,EAAlC,CAAH;AACA,UAAMb,KAAK,GAAG,MAAMvE,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoB6B,GAApB,CAAwBxB,IAAI,CAACsB,IAA7B,CAApB;AACA,UAAMzD,IAAI,GAAG1B,KAAK,CAACsF,MAAN,CAAaF,KAAb,CAAb,CA/BoC,CAiCpC;;AACA,QAAI,CAACY,IAAI,CAACM,CAAC,GAAG,CAAL,CAAT,EAAkB;AAChB3F,MAAAA,GAAG,CAAE,uBAAsB4F,OAAO,CAACN,MAAO,EAAvC,CAAH;AAEA,YAAMjC,iBAAiB,CAACnD,OAAD,EAAUa,IAAI,CAACc,KAAf,EAAsByB,UAAtB,EAAkCsC,OAAO,CAACxF,MAA1C,EAAkDgE,QAAQ,CAACwB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAA1D,CAAvB;AACA,YAAMT,QAAQ,GAAG,MAAMvB,UAAU,CAAC8B,oBAAX,CAAgCF,QAAhC,CAAvB,CAJgB,CAMhB;;AACAG,MAAAA,IAAI,CAACG,IAAL,CAAU;AACRpF,QAAAA,MAAM,EAAEyE,QAAQ,CAACzE,MADT;AAERkF,QAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACV,GAAV,CAFR;AAGRpD,QAAAA,IAAI,EAAEA;AAHE,OAAV;AAMA;AACD;;AAED,UAAMgF,WAAW,GAAGV,IAAI,CAACM,CAAC,GAAG,CAAL,CAAxB,CAlDoC,CAoDpC;;AACA,UAAM/B,oBAAoB,CAAC1D,OAAD,EAAUa,IAAI,CAACc,KAAf,EAAsBkE,WAAW,CAAC3F,MAAlC,EAA0CkD,UAA1C,CAA1B;AAEAyC,IAAAA,WAAW,CAAChF,IAAZ,GAAmBA,IAAnB;AACD;;AAED,QAAMuC,UAAU,CAACR,GAAX,CAAeoC,QAAf,EAAyB,IAAzB,CAAN;AAEAG,EAAAA,IAAI,CAACvE,OAAL;AAEA,SAAO;AACLwC,IAAAA,UADK;AAEL+B,IAAAA;AAFK,GAAP;AAID,CA7FM;AA+FP;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,OAAO,MAAMW,WAAW,GAAG,gBAAO9F,OAAP,EAAgB+F,QAAhB,EAA2C;AAAA,MAAjB5F,OAAiB,uEAAP,EAAO;AACpE,QAAM6F,KAAK,GAAG,IAAI1G,UAAJ,CAAe;AAC3B2G,IAAAA,IAAI,EAAE,IADqB;AAE3BlF,IAAAA,GAAG,EAAE,IAFsB;AAG3BX,IAAAA,MAAM,EAAE8F,SAHmB;AAI3BC,IAAAA,SAAS,EAAED,SAJgB;AAK3Bf,IAAAA,IAAI,EAAE,EALqB;AAM3BiB,IAAAA,KAAK,EAAE,IANoB;AAO3BC,IAAAA,IAAI,EAAE,KAPqB;AAQ3BhF,IAAAA,KAAK,EAAElB,OAAO,CAACkB,KARY;AAS3BD,IAAAA,IAAI,EAAEjB,OAAO,CAACiB;AATa,GAAf,EAUXjB,OAVW,CAAd;;AAYA,OAAK,IAAIsF,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGM,QAAQ,CAAC7C,MAA7B,EAAqCuC,CAAC,EAAtC,EAA0C;AACxC,UAAMO,KAAK,CAACM,OAAN,CAAc1D,GAAd,CAAkBmD,QAAQ,CAACN,CAAD,CAAR,CAAYc,IAA9B,EAAoC;AACxC1D,MAAAA,IAAI,EAAEkD,QAAQ,CAACN,CAAD,CAAR,CAAY5C,IADsB;AAExCR,MAAAA,GAAG,EAAE0D,QAAQ,CAACN,CAAD,CAAR,CAAYpD;AAFuB,KAApC,CAAN;AAID;;AAED,QAAMmE,GAAG,GAAG,MAAM/G,IAAI,CAACuG,KAAK,CAACvD,KAAN,CAAYzC,OAAO,CAAC0C,IAAR,CAAaC,MAAzB,CAAD,CAAtB;;AAEA,MAAI,CAAC6D,GAAL,EAAU;AACR,UAAM,IAAIlG,KAAJ,CAAU,kCAAV,CAAN;AACD;;AAED,SAAOkG,GAAP;AACD,CA3BM","sourcesContent":["import * as dagPB from '@ipld/dag-pb'\nimport {\n  Bucket,\n  createHAMT\n} from 'hamt-sharding'\nimport { DirSharded } from './dir-sharded.js'\nimport { logger } from '@libp2p/logger'\nimport { UnixFS } from 'ipfs-unixfs'\nimport last from 'it-last'\nimport { CID } from 'multiformats/cid'\nimport {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} from './hamt-constants.js'\n\nconst log = logger('ipfs:mfs:core:utils:hamt-utils')\n\n/**\n * @typedef {import('multiformats/cid').Version} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\nexport const updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data')\n  }\n\n  // update parent with new bit field\n  const data = Uint8Array.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hasher = await context.hashers.getHasher(options.hashAlg)\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  }\n  const buf = dagPB.encode(parent)\n  const hash = await hasher.digest(buf)\n  const cid = CID.create(options.cidVersion, dagPB.code, hash)\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf)\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\nexport const recreateHamtLevel = async (context, links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent)\n  parentBucket._putObjectAt(positionAtParent, bucket)\n\n  await addLinksToHamtBucket(context, links, bucket, rootBucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n */\nexport const recreateInitialHamtLevel = async (links) => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  })\n\n  // populate sub bucket but do not recurse as we do not want to pull whole shard in\n  await Promise.all(\n    links.map(async link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16)\n\n        const subBucket = new Bucket({\n          hash: bucket._options.hash,\n          bits: bucket._options.bits\n        }, bucket, pos)\n        bucket._putObjectAt(pos, subBucket)\n\n        return Promise.resolve()\n      }\n\n      return bucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n\n  return bucket\n}\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\nexport const addLinksToHamtBucket = async (context, links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(async link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        log('Populating sub bucket', linkName)\n        const pos = parseInt(linkName, 16)\n        const block = await context.repo.blocks.get(link.Hash)\n        const node = dagPB.decode(block)\n\n        const subBucket = new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos)\n        bucket._putObjectAt(pos, subBucket)\n\n        await addLinksToHamtBucket(context, node.Links, subBucket, rootBucket)\n\n        return Promise.resolve()\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\n/**\n * @param {number} position\n */\nexport const toPrefix = (position) => {\n  return position\n    .toString(16)\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\nexport const generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    // @ts-expect-error - only the root bucket's parent will be undefined\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load PbNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path')\n    }\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => (link.Name || '').substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const block = await context.repo.blocks.get(link.Hash)\n    const node = dagPB.decode(block)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(context, node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(context, node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nexport const createShard = async (context, contents, options = {}) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  const res = await last(shard.flush(context.repo.blocks))\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result')\n  }\n\n  return res\n}\n"]},"metadata":{},"sourceType":"module"}