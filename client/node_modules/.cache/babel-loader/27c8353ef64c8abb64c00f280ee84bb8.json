{"ast":null,"code":"import errCode from 'err-code';\nimport { UnixFS } from 'ipfs-unixfs';\nimport persist from '../../utils/persist.js';\nimport { encode, prepare } from '@ipld/dag-pb';\nimport parallelBatch from 'it-parallel-batch';\nimport * as rawCodec from 'multiformats/codecs/raw';\nimport * as dagPb from '@ipld/dag-pb';\nimport dagFlat from './flat.js';\nimport dagBalanced from './balanced.js';\nimport dagTrickle from './trickle.js';\nimport bufferImporterFn from './buffer-importer.js';\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\n\nconst dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n};\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\n\nasync function* buildFileBatch(file, blockstore, options) {\n  let count = -1;\n  let previous;\n  let bufferImporter;\n\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter;\n  } else {\n    bufferImporter = bufferImporterFn;\n  }\n\n  for await (const entry of parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency)) {\n    count++;\n\n    if (count === 0) {\n      previous = entry;\n      continue;\n    } else if (count === 1 && previous) {\n      yield previous;\n      previous = null;\n    }\n\n    yield entry;\n  }\n\n  if (previous) {\n    previous.single = true;\n    yield previous;\n  }\n}\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\n\n\nconst reduce = (file, blockstore, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer(leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0];\n\n      if (file.mtime !== undefined || file.mode !== undefined) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let buffer = await blockstore.get(leaf.cid);\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        });\n        buffer = encode(prepare({\n          Data: leaf.unixfs.marshal()\n        })); // // TODO vmx 2021-03-26: This is what the original code does, it checks\n        // // the multihash of the original leaf node and uses then the same\n        // // hasher. i wonder if that's really needed or if we could just use\n        // // the hasher from `options.hasher` instead.\n        // const multihash = mh.decode(leaf.cid.multihash.bytes)\n        // let hasher\n        // switch multihash {\n        //   case sha256.code {\n        //     hasher = sha256\n        //     break;\n        //   }\n        //   //case identity.code {\n        //   //  hasher = identity\n        //   //  break;\n        //   //}\n        //   default: {\n        //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n        //   }\n        // }\n\n        leaf.cid = await persist(buffer, blockstore, { ...options,\n          codec: dagPb,\n          hasher: options.hasher,\n          cidVersion: options.cidVersion\n        });\n        leaf.size = buffer.length;\n      }\n\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      };\n    } // create a parent node and add all the leaves\n\n\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    });\n    const links = leaves.filter(leaf => {\n      if (leaf.cid.code === rawCodec.code && leaf.size) {\n        return true;\n      }\n\n      if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n        return true;\n      }\n\n      return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length);\n    }).map(leaf => {\n      if (leaf.cid.code === rawCodec.code) {\n        // node is a leaf buffer\n        f.addBlockSize(leaf.size);\n        return {\n          Name: '',\n          Tsize: leaf.size,\n          Hash: leaf.cid\n        };\n      }\n\n      if (!leaf.unixfs || !leaf.unixfs.data) {\n        // node is an intermediate node\n        f.addBlockSize(leaf.unixfs && leaf.unixfs.fileSize() || 0);\n      } else {\n        // node is a unixfs 'file' leaf node\n        f.addBlockSize(leaf.unixfs.data.length);\n      }\n\n      return {\n        Name: '',\n        Tsize: leaf.size,\n        Hash: leaf.cid\n      };\n    });\n    const node = {\n      Data: f.marshal(),\n      Links: links\n    };\n    const buffer = encode(prepare(node));\n    const cid = await persist(buffer, blockstore, options);\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    };\n  }\n\n  return reducer;\n};\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\n\n\nfunction fileBuilder(file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy];\n\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY');\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options);\n}\n\nexport default fileBuilder;","map":{"version":3,"sources":["C:/Users/Akshay Mishra/OneDrive/Desktop/twitter-clone-dapp/node_modules/ipfs-unixfs-importer/src/dag-builder/file/index.js"],"names":["errCode","UnixFS","persist","encode","prepare","parallelBatch","rawCodec","dagPb","dagFlat","dagBalanced","dagTrickle","bufferImporterFn","dagBuilders","flat","balanced","trickle","buildFileBatch","file","blockstore","options","count","previous","bufferImporter","entry","blockWriteConcurrency","single","reduce","reducer","leaves","length","reduceSingleLeafToSelf","leaf","mtime","undefined","mode","buffer","get","cid","unixfs","type","data","Data","marshal","codec","hasher","cidVersion","size","path","f","links","filter","code","fileSize","Boolean","map","addBlockSize","Name","Tsize","Hash","node","Links","acc","curr","fileBuilder","block","dagBuilder","strategy","Error"],"mappings":"AAAA,OAAOA,OAAP,MAAoB,UAApB;AACA,SAASC,MAAT,QAAuB,aAAvB;AACA,OAAOC,OAAP,MAAoB,wBAApB;AACA,SAASC,MAAT,EAAiBC,OAAjB,QAAgC,cAAhC;AACA,OAAOC,aAAP,MAA0B,mBAA1B;AACA,OAAO,KAAKC,QAAZ,MAA0B,yBAA1B;AACA,OAAO,KAAKC,KAAZ,MAAuB,cAAvB;AAEA,OAAOC,OAAP,MAAoB,WAApB;AACA,OAAOC,WAAP,MAAwB,eAAxB;AACA,OAAOC,UAAP,MAAuB,cAAvB;AACA,OAAOC,gBAAP,MAA6B,sBAA7B;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AACA,MAAMC,WAAW,GAAG;AAClBC,EAAAA,IAAI,EAAEL,OADY;AAElBM,EAAAA,QAAQ,EAAEL,WAFQ;AAGlBM,EAAAA,OAAO,EAAEL;AAHS,CAApB;AAMA;AACA;AACA;AACA;AACA;;AACA,gBAAiBM,cAAjB,CAAiCC,IAAjC,EAAuCC,UAAvC,EAAmDC,OAAnD,EAA4D;AAC1D,MAAIC,KAAK,GAAG,CAAC,CAAb;AACA,MAAIC,QAAJ;AACA,MAAIC,cAAJ;;AAEA,MAAI,OAAOH,OAAO,CAACG,cAAf,KAAkC,UAAtC,EAAkD;AAChDA,IAAAA,cAAc,GAAGH,OAAO,CAACG,cAAzB;AACD,GAFD,MAEO;AACLA,IAAAA,cAAc,GAAGX,gBAAjB;AACD;;AAED,aAAW,MAAMY,KAAjB,IAA0BlB,aAAa,CAACiB,cAAc,CAACL,IAAD,EAAOC,UAAP,EAAmBC,OAAnB,CAAf,EAA4CA,OAAO,CAACK,qBAApD,CAAvC,EAAmH;AACjHJ,IAAAA,KAAK;;AAEL,QAAIA,KAAK,KAAK,CAAd,EAAiB;AACfC,MAAAA,QAAQ,GAAGE,KAAX;AACA;AACD,KAHD,MAGO,IAAIH,KAAK,KAAK,CAAV,IAAeC,QAAnB,EAA6B;AAClC,YAAMA,QAAN;AACAA,MAAAA,QAAQ,GAAG,IAAX;AACD;;AAED,UAAME,KAAN;AACD;;AAED,MAAIF,QAAJ,EAAc;AACZA,IAAAA,QAAQ,CAACI,MAAT,GAAkB,IAAlB;AACA,UAAMJ,QAAN;AACD;AACF;AAED;AACA;AACA;AACA;AACA;;;AACA,MAAMK,MAAM,GAAG,CAACT,IAAD,EAAOC,UAAP,EAAmBC,OAAnB,KAA+B;AAC5C;AACF;AACA;AACE,iBAAeQ,OAAf,CAAwBC,MAAxB,EAAgC;AAC9B,QAAIA,MAAM,CAACC,MAAP,KAAkB,CAAlB,IAAuBD,MAAM,CAAC,CAAD,CAAN,CAAUH,MAAjC,IAA2CN,OAAO,CAACW,sBAAvD,EAA+E;AAC7E,YAAMC,IAAI,GAAGH,MAAM,CAAC,CAAD,CAAnB;;AAEA,UAAIX,IAAI,CAACe,KAAL,KAAeC,SAAf,IAA4BhB,IAAI,CAACiB,IAAL,KAAcD,SAA9C,EAAyD;AACvD;AACA;AACA,YAAIE,MAAM,GAAG,MAAMjB,UAAU,CAACkB,GAAX,CAAeL,IAAI,CAACM,GAApB,CAAnB;AAEAN,QAAAA,IAAI,CAACO,MAAL,GAAc,IAAIrC,MAAJ,CAAW;AACvBsC,UAAAA,IAAI,EAAE,MADiB;AAEvBP,UAAAA,KAAK,EAAEf,IAAI,CAACe,KAFW;AAGvBE,UAAAA,IAAI,EAAEjB,IAAI,CAACiB,IAHY;AAIvBM,UAAAA,IAAI,EAAEL;AAJiB,SAAX,CAAd;AAOAA,QAAAA,MAAM,GAAGhC,MAAM,CAACC,OAAO,CAAC;AAAEqC,UAAAA,IAAI,EAAEV,IAAI,CAACO,MAAL,CAAYI,OAAZ;AAAR,SAAD,CAAR,CAAf,CAZuD,CAcvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACAX,QAAAA,IAAI,CAACM,GAAL,GAAW,MAAMnC,OAAO,CAACiC,MAAD,EAASjB,UAAT,EAAqB,EAC3C,GAAGC,OADwC;AAE3CwB,UAAAA,KAAK,EAAEpC,KAFoC;AAG3CqC,UAAAA,MAAM,EAAEzB,OAAO,CAACyB,MAH2B;AAI3CC,UAAAA,UAAU,EAAE1B,OAAO,CAAC0B;AAJuB,SAArB,CAAxB;AAMAd,QAAAA,IAAI,CAACe,IAAL,GAAYX,MAAM,CAACN,MAAnB;AACD;;AAED,aAAO;AACLQ,QAAAA,GAAG,EAAEN,IAAI,CAACM,GADL;AAELU,QAAAA,IAAI,EAAE9B,IAAI,CAAC8B,IAFN;AAGLT,QAAAA,MAAM,EAAEP,IAAI,CAACO,MAHR;AAILQ,QAAAA,IAAI,EAAEf,IAAI,CAACe;AAJN,OAAP;AAMD,KApD6B,CAsD9B;;;AACA,UAAME,CAAC,GAAG,IAAI/C,MAAJ,CAAW;AACnBsC,MAAAA,IAAI,EAAE,MADa;AAEnBP,MAAAA,KAAK,EAAEf,IAAI,CAACe,KAFO;AAGnBE,MAAAA,IAAI,EAAEjB,IAAI,CAACiB;AAHQ,KAAX,CAAV;AAMA,UAAMe,KAAK,GAAGrB,MAAM,CACjBsB,MADW,CACJnB,IAAI,IAAI;AACd,UAAIA,IAAI,CAACM,GAAL,CAASc,IAAT,KAAkB7C,QAAQ,CAAC6C,IAA3B,IAAmCpB,IAAI,CAACe,IAA5C,EAAkD;AAChD,eAAO,IAAP;AACD;;AAED,UAAIf,IAAI,CAACO,MAAL,IAAe,CAACP,IAAI,CAACO,MAAL,CAAYE,IAA5B,IAAoCT,IAAI,CAACO,MAAL,CAAYc,QAAZ,EAAxC,EAAgE;AAC9D,eAAO,IAAP;AACD;;AAED,aAAOC,OAAO,CAACtB,IAAI,CAACO,MAAL,IAAeP,IAAI,CAACO,MAAL,CAAYE,IAA3B,IAAmCT,IAAI,CAACO,MAAL,CAAYE,IAAZ,CAAiBX,MAArD,CAAd;AACD,KAXW,EAYXyB,GAZW,CAYNvB,IAAD,IAAU;AACb,UAAIA,IAAI,CAACM,GAAL,CAASc,IAAT,KAAkB7C,QAAQ,CAAC6C,IAA/B,EAAqC;AACnC;AACAH,QAAAA,CAAC,CAACO,YAAF,CAAexB,IAAI,CAACe,IAApB;AAEA,eAAO;AACLU,UAAAA,IAAI,EAAE,EADD;AAELC,UAAAA,KAAK,EAAE1B,IAAI,CAACe,IAFP;AAGLY,UAAAA,IAAI,EAAE3B,IAAI,CAACM;AAHN,SAAP;AAKD;;AAED,UAAI,CAACN,IAAI,CAACO,MAAN,IAAgB,CAACP,IAAI,CAACO,MAAL,CAAYE,IAAjC,EAAuC;AACrC;AACAQ,QAAAA,CAAC,CAACO,YAAF,CAAgBxB,IAAI,CAACO,MAAL,IAAeP,IAAI,CAACO,MAAL,CAAYc,QAAZ,EAAhB,IAA2C,CAA1D;AACD,OAHD,MAGO;AACL;AACAJ,QAAAA,CAAC,CAACO,YAAF,CAAexB,IAAI,CAACO,MAAL,CAAYE,IAAZ,CAAiBX,MAAhC;AACD;;AAED,aAAO;AACL2B,QAAAA,IAAI,EAAE,EADD;AAELC,QAAAA,KAAK,EAAE1B,IAAI,CAACe,IAFP;AAGLY,QAAAA,IAAI,EAAE3B,IAAI,CAACM;AAHN,OAAP;AAKD,KArCW,CAAd;AAuCA,UAAMsB,IAAI,GAAG;AACXlB,MAAAA,IAAI,EAAEO,CAAC,CAACN,OAAF,EADK;AAEXkB,MAAAA,KAAK,EAAEX;AAFI,KAAb;AAIA,UAAMd,MAAM,GAAGhC,MAAM,CAACC,OAAO,CAACuD,IAAD,CAAR,CAArB;AACA,UAAMtB,GAAG,GAAG,MAAMnC,OAAO,CAACiC,MAAD,EAASjB,UAAT,EAAqBC,OAArB,CAAzB;AAEA,WAAO;AACLkB,MAAAA,GADK;AAELU,MAAAA,IAAI,EAAE9B,IAAI,CAAC8B,IAFN;AAGLT,MAAAA,MAAM,EAAEU,CAHH;AAILF,MAAAA,IAAI,EAAEX,MAAM,CAACN,MAAP,GAAgB8B,IAAI,CAACC,KAAL,CAAWlC,MAAX,CAAkB,CAACmC,GAAD,EAAMC,IAAN,KAAeD,GAAG,GAAGC,IAAI,CAACL,KAA5C,EAAmD,CAAnD;AAJjB,KAAP;AAMD;;AAED,SAAO9B,OAAP;AACD,CAxHD;AA0HA;AACA;AACA;;;AACA,SAASoC,WAAT,CAAsB9C,IAAtB,EAA4B+C,KAA5B,EAAmC7C,OAAnC,EAA4C;AAC1C,QAAM8C,UAAU,GAAGrD,WAAW,CAACO,OAAO,CAAC+C,QAAT,CAA9B;;AAEA,MAAI,CAACD,UAAL,EAAiB;AACf,UAAMjE,OAAO,CAAC,IAAImE,KAAJ,CAAW,yCAAwChD,OAAO,CAAC+C,QAAS,EAApE,CAAD,EAAyE,kBAAzE,CAAb;AACD;;AAED,SAAOD,UAAU,CAACjD,cAAc,CAACC,IAAD,EAAO+C,KAAP,EAAc7C,OAAd,CAAf,EAAuCO,MAAM,CAACT,IAAD,EAAO+C,KAAP,EAAc7C,OAAd,CAA7C,EAAqEA,OAArE,CAAjB;AACD;;AAED,eAAe4C,WAAf","sourcesContent":["import errCode from 'err-code'\nimport { UnixFS } from 'ipfs-unixfs'\nimport persist from '../../utils/persist.js'\nimport { encode, prepare } from '@ipld/dag-pb'\nimport parallelBatch from 'it-parallel-batch'\nimport * as rawCodec from 'multiformats/codecs/raw'\nimport * as dagPb from '@ipld/dag-pb'\n\nimport dagFlat from './flat.js'\nimport dagBalanced from './balanced.js'\nimport dagTrickle from './trickle.js'\nimport bufferImporterFn from './buffer-importer.js'\n\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\nconst dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nasync function * buildFileBatch (file, blockstore, options) {\n  let count = -1\n  let previous\n  let bufferImporter\n\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter\n  } else {\n    bufferImporter = bufferImporterFn\n  }\n\n  for await (const entry of parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency)) {\n    count++\n\n    if (count === 0) {\n      previous = entry\n      continue\n    } else if (count === 1 && previous) {\n      yield previous\n      previous = null\n    }\n\n    yield entry\n  }\n\n  if (previous) {\n    previous.single = true\n    yield previous\n  }\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nconst reduce = (file, blockstore, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer (leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0]\n\n      if (file.mtime !== undefined || file.mode !== undefined) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let buffer = await blockstore.get(leaf.cid)\n\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        })\n\n        buffer = encode(prepare({ Data: leaf.unixfs.marshal() }))\n\n        // // TODO vmx 2021-03-26: This is what the original code does, it checks\n        // // the multihash of the original leaf node and uses then the same\n        // // hasher. i wonder if that's really needed or if we could just use\n        // // the hasher from `options.hasher` instead.\n        // const multihash = mh.decode(leaf.cid.multihash.bytes)\n        // let hasher\n        // switch multihash {\n        //   case sha256.code {\n        //     hasher = sha256\n        //     break;\n        //   }\n        //   //case identity.code {\n        //   //  hasher = identity\n        //   //  break;\n        //   //}\n        //   default: {\n        //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n        //   }\n        // }\n        leaf.cid = await persist(buffer, blockstore, {\n          ...options,\n          codec: dagPb,\n          hasher: options.hasher,\n          cidVersion: options.cidVersion\n        })\n        leaf.size = buffer.length\n      }\n\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      }\n    }\n\n    // create a parent node and add all the leaves\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    })\n\n    const links = leaves\n      .filter(leaf => {\n        if (leaf.cid.code === rawCodec.code && leaf.size) {\n          return true\n        }\n\n        if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n          return true\n        }\n\n        return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length)\n      })\n      .map((leaf) => {\n        if (leaf.cid.code === rawCodec.code) {\n          // node is a leaf buffer\n          f.addBlockSize(leaf.size)\n\n          return {\n            Name: '',\n            Tsize: leaf.size,\n            Hash: leaf.cid\n          }\n        }\n\n        if (!leaf.unixfs || !leaf.unixfs.data) {\n          // node is an intermediate node\n          f.addBlockSize((leaf.unixfs && leaf.unixfs.fileSize()) || 0)\n        } else {\n          // node is a unixfs 'file' leaf node\n          f.addBlockSize(leaf.unixfs.data.length)\n        }\n\n        return {\n          Name: '',\n          Tsize: leaf.size,\n          Hash: leaf.cid\n        }\n      })\n\n    const node = {\n      Data: f.marshal(),\n      Links: links\n    }\n    const buffer = encode(prepare(node))\n    const cid = await persist(buffer, blockstore, options)\n\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    }\n  }\n\n  return reducer\n}\n\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\nfunction fileBuilder (file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy]\n\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY')\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options)\n}\n\nexport default fileBuilder\n"]},"metadata":{},"sourceType":"module"}