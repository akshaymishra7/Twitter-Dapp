{"ast":null,"code":"import extractDataFromBlock from '../../../utils/extract-data-from-block.js';\nimport validateOffsetAndLength from '../../../utils/validate-offset-and-length.js';\nimport { UnixFS } from 'ipfs-unixfs';\nimport errCode from 'err-code';\nimport * as dagPb from '@ipld/dag-pb';\nimport * as raw from 'multiformats/codecs/raw';\nimport { pushable } from 'it-pushable';\nimport parallel from 'it-parallel';\nimport { pipe } from 'it-pipe';\nimport map from 'it-map';\nimport PQueue from 'p-queue';\n/**\n * @typedef {import('../../../types').ExporterOptions} ExporterOptions\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {Blockstore} blockstore\n * @param {PBNode | Uint8Array} node\n * @param {import('it-pushable').Pushable<Uint8Array>} queue\n * @param {number} streamPosition\n * @param {number} start\n * @param {number} end\n * @param {PQueue} walkQueue\n * @param {ExporterOptions} options\n * @returns {Promise<void>}\n */\n\nasync function walkDAG(blockstore, node, queue, streamPosition, start, end, walkQueue, options) {\n  // a `raw` node\n  if (node instanceof Uint8Array) {\n    queue.push(extractDataFromBlock(node, streamPosition, start, end));\n    return;\n  }\n\n  if (node.Data == null) {\n    throw errCode(new Error('no data in PBNode'), 'ERR_NOT_UNIXFS');\n  }\n  /** @type {UnixFS} */\n\n\n  let file;\n\n  try {\n    file = UnixFS.unmarshal(node.Data);\n  } catch (\n  /** @type {any} */\n  err) {\n    throw errCode(err, 'ERR_NOT_UNIXFS');\n  } // might be a unixfs `raw` node or have data on intermediate nodes\n\n\n  if (file.data != null) {\n    const data = file.data;\n    const buf = extractDataFromBlock(data, streamPosition, start, end);\n    queue.push(buf);\n    streamPosition += buf.byteLength;\n  }\n  /** @type {Array<{ link: PBLink, blockStart: number }>} */\n\n\n  const childOps = [];\n\n  for (let i = 0; i < node.Links.length; i++) {\n    const childLink = node.Links[i];\n    const childStart = streamPosition; // inclusive\n\n    const childEnd = childStart + file.blockSizes[i]; // exclusive\n\n    if (start >= childStart && start < childEnd || // child has offset byte\n    end >= childStart && end <= childEnd || // child has end byte\n    start < childStart && end > childEnd) {\n      // child is between offset and end bytes\n      childOps.push({\n        link: childLink,\n        blockStart: streamPosition\n      });\n    }\n\n    streamPosition = childEnd;\n\n    if (streamPosition > end) {\n      break;\n    }\n  }\n\n  await pipe(childOps, source => map(source, op => {\n    return async () => {\n      const block = await blockstore.get(op.link.Hash, {\n        signal: options.signal\n      });\n      return { ...op,\n        block\n      };\n    };\n  }), source => parallel(source, {\n    ordered: true\n  }), async source => {\n    for await (const {\n      link,\n      block,\n      blockStart\n    } of source) {\n      /** @type {PBNode | Uint8Array} */\n      let child;\n\n      switch (link.Hash.code) {\n        case dagPb.code:\n          child = dagPb.decode(block);\n          break;\n\n        case raw.code:\n          child = block;\n          break;\n\n        default:\n          queue.end(errCode(new Error(`Unsupported codec: ${link.Hash.code}`), 'ERR_NOT_UNIXFS'));\n          return;\n      }\n\n      walkQueue.add(async () => {\n        await walkDAG(blockstore, child, queue, blockStart, start, end, walkQueue, options);\n      });\n    }\n  });\n}\n/**\n * @type {import('../').UnixfsV1Resolver}\n */\n\n\nconst fileContent = (cid, node, unixfs, path, resolve, depth, blockstore) => {\n  /**\n   * @param {ExporterOptions} options\n   */\n  async function* yieldFileContent() {\n    let options = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    const fileSize = unixfs.fileSize();\n\n    if (fileSize === undefined) {\n      throw new Error('File was a directory');\n    }\n\n    const {\n      offset,\n      length\n    } = validateOffsetAndLength(fileSize, options.offset, options.length);\n\n    if (length === 0) {\n      return;\n    } // use a queue to walk the DAG instead of recursion to ensure very deep DAGs\n    // don't overflow the stack\n\n\n    const walkQueue = new PQueue({\n      concurrency: 1\n    });\n    const queue = pushable();\n    walkQueue.add(async () => {\n      await walkDAG(blockstore, node, queue, 0, offset, offset + length, walkQueue, options);\n    });\n    walkQueue.on('error', error => {\n      queue.end(error);\n    });\n    let read = 0;\n\n    for await (const buf of queue) {\n      if (buf == null) {\n        continue;\n      }\n\n      read += buf.byteLength;\n\n      if (read === length) {\n        queue.end();\n      }\n\n      yield buf;\n    }\n  }\n\n  return yieldFileContent;\n};\n\nexport default fileContent;","map":{"version":3,"sources":["C:/Users/Akshay Mishra/OneDrive/Desktop/twitter-clone-dapp/node_modules/ipfs-unixfs-exporter/src/resolvers/unixfs-v1/content/file.js"],"names":["extractDataFromBlock","validateOffsetAndLength","UnixFS","errCode","dagPb","raw","pushable","parallel","pipe","map","PQueue","walkDAG","blockstore","node","queue","streamPosition","start","end","walkQueue","options","Uint8Array","push","Data","Error","file","unmarshal","err","data","buf","byteLength","childOps","i","Links","length","childLink","childStart","childEnd","blockSizes","link","blockStart","source","op","block","get","Hash","signal","ordered","child","code","decode","add","fileContent","cid","unixfs","path","resolve","depth","yieldFileContent","fileSize","undefined","offset","concurrency","on","error","read"],"mappings":"AAAA,OAAOA,oBAAP,MAAiC,2CAAjC;AACA,OAAOC,uBAAP,MAAoC,8CAApC;AACA,SAASC,MAAT,QAAuB,aAAvB;AACA,OAAOC,OAAP,MAAoB,UAApB;AACA,OAAO,KAAKC,KAAZ,MAAuB,cAAvB;AACA,OAAO,KAAKC,GAAZ,MAAqB,yBAArB;AACA,SAASC,QAAT,QAAyB,aAAzB;AACA,OAAOC,QAAP,MAAqB,aAArB;AACA,SAASC,IAAT,QAAqB,SAArB;AACA,OAAOC,GAAP,MAAgB,QAAhB;AACA,OAAOC,MAAP,MAAmB,SAAnB;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,eAAeC,OAAf,CAAwBC,UAAxB,EAAoCC,IAApC,EAA0CC,KAA1C,EAAiDC,cAAjD,EAAiEC,KAAjE,EAAwEC,GAAxE,EAA6EC,SAA7E,EAAwFC,OAAxF,EAAiG;AAC/F;AACA,MAAIN,IAAI,YAAYO,UAApB,EAAgC;AAC9BN,IAAAA,KAAK,CAACO,IAAN,CAAWrB,oBAAoB,CAACa,IAAD,EAAOE,cAAP,EAAuBC,KAAvB,EAA8BC,GAA9B,CAA/B;AAEA;AACD;;AAED,MAAIJ,IAAI,CAACS,IAAL,IAAa,IAAjB,EAAuB;AACrB,UAAMnB,OAAO,CAAC,IAAIoB,KAAJ,CAAU,mBAAV,CAAD,EAAiC,gBAAjC,CAAb;AACD;AAED;;;AACA,MAAIC,IAAJ;;AAEA,MAAI;AACFA,IAAAA,IAAI,GAAGtB,MAAM,CAACuB,SAAP,CAAiBZ,IAAI,CAACS,IAAtB,CAAP;AACD,GAFD,CAEE;AAAO;AAAmBI,EAAAA,GAA1B,EAA+B;AAC/B,UAAMvB,OAAO,CAACuB,GAAD,EAAM,gBAAN,CAAb;AACD,GAnB8F,CAqB/F;;;AACA,MAAIF,IAAI,CAACG,IAAL,IAAa,IAAjB,EAAuB;AACrB,UAAMA,IAAI,GAAGH,IAAI,CAACG,IAAlB;AACA,UAAMC,GAAG,GAAG5B,oBAAoB,CAAC2B,IAAD,EAAOZ,cAAP,EAAuBC,KAAvB,EAA8BC,GAA9B,CAAhC;AAEAH,IAAAA,KAAK,CAACO,IAAN,CAAWO,GAAX;AAEAb,IAAAA,cAAc,IAAIa,GAAG,CAACC,UAAtB;AACD;AAED;;;AACA,QAAMC,QAAQ,GAAG,EAAjB;;AAEA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGlB,IAAI,CAACmB,KAAL,CAAWC,MAA/B,EAAuCF,CAAC,EAAxC,EAA4C;AAC1C,UAAMG,SAAS,GAAGrB,IAAI,CAACmB,KAAL,CAAWD,CAAX,CAAlB;AACA,UAAMI,UAAU,GAAGpB,cAAnB,CAF0C,CAER;;AAClC,UAAMqB,QAAQ,GAAGD,UAAU,GAAGX,IAAI,CAACa,UAAL,CAAgBN,CAAhB,CAA9B,CAH0C,CAGO;;AAEjD,QAAKf,KAAK,IAAImB,UAAT,IAAuBnB,KAAK,GAAGoB,QAAhC,IAA6C;AAC5CnB,IAAAA,GAAG,IAAIkB,UAAP,IAAqBlB,GAAG,IAAImB,QAD7B,IAC0C;AACzCpB,IAAAA,KAAK,GAAGmB,UAAR,IAAsBlB,GAAG,GAAGmB,QAFjC,EAE4C;AAAE;AAC5CN,MAAAA,QAAQ,CAACT,IAAT,CAAc;AACZiB,QAAAA,IAAI,EAAEJ,SADM;AAEZK,QAAAA,UAAU,EAAExB;AAFA,OAAd;AAID;;AAEDA,IAAAA,cAAc,GAAGqB,QAAjB;;AAEA,QAAIrB,cAAc,GAAGE,GAArB,EAA0B;AACxB;AACD;AACF;;AAED,QAAMT,IAAI,CACRsB,QADQ,EAEPU,MAAD,IAAY/B,GAAG,CAAC+B,MAAD,EAAUC,EAAD,IAAQ;AAC9B,WAAO,YAAY;AACjB,YAAMC,KAAK,GAAG,MAAM9B,UAAU,CAAC+B,GAAX,CAAeF,EAAE,CAACH,IAAH,CAAQM,IAAvB,EAA6B;AAC/CC,QAAAA,MAAM,EAAE1B,OAAO,CAAC0B;AAD+B,OAA7B,CAApB;AAIA,aAAO,EACL,GAAGJ,EADE;AAELC,QAAAA;AAFK,OAAP;AAID,KATD;AAUD,GAXc,CAFP,EAcPF,MAAD,IAAYjC,QAAQ,CAACiC,MAAD,EAAS;AAC3BM,IAAAA,OAAO,EAAE;AADkB,GAAT,CAdZ,EAiBR,MAAON,MAAP,IAAkB;AAChB,eAAW,MAAM;AAAEF,MAAAA,IAAF;AAAQI,MAAAA,KAAR;AAAeH,MAAAA;AAAf,KAAjB,IAAgDC,MAAhD,EAAwD;AACtD;AACA,UAAIO,KAAJ;;AACA,cAAQT,IAAI,CAACM,IAAL,CAAUI,IAAlB;AACE,aAAK5C,KAAK,CAAC4C,IAAX;AACED,UAAAA,KAAK,GAAG3C,KAAK,CAAC6C,MAAN,CAAaP,KAAb,CAAR;AACA;;AACF,aAAKrC,GAAG,CAAC2C,IAAT;AACED,UAAAA,KAAK,GAAGL,KAAR;AACA;;AACF;AACE5B,UAAAA,KAAK,CAACG,GAAN,CAAUd,OAAO,CAAC,IAAIoB,KAAJ,CAAW,sBAAqBe,IAAI,CAACM,IAAL,CAAUI,IAAK,EAA/C,CAAD,EAAoD,gBAApD,CAAjB;AACA;AATJ;;AAYA9B,MAAAA,SAAS,CAACgC,GAAV,CAAc,YAAY;AACxB,cAAMvC,OAAO,CAACC,UAAD,EAAamC,KAAb,EAAoBjC,KAApB,EAA2ByB,UAA3B,EAAuCvB,KAAvC,EAA8CC,GAA9C,EAAmDC,SAAnD,EAA8DC,OAA9D,CAAb;AACD,OAFD;AAGD;AACF,GArCO,CAAV;AAuCD;AAED;AACA;AACA;;;AACA,MAAMgC,WAAW,GAAG,CAACC,GAAD,EAAMvC,IAAN,EAAYwC,MAAZ,EAAoBC,IAApB,EAA0BC,OAA1B,EAAmCC,KAAnC,EAA0C5C,UAA1C,KAAyD;AAC3E;AACF;AACA;AACE,kBAAiB6C,gBAAjB,GAAiD;AAAA,QAAdtC,OAAc,uEAAJ,EAAI;AAC/C,UAAMuC,QAAQ,GAAGL,MAAM,CAACK,QAAP,EAAjB;;AAEA,QAAIA,QAAQ,KAAKC,SAAjB,EAA4B;AAC1B,YAAM,IAAIpC,KAAJ,CAAU,sBAAV,CAAN;AACD;;AAED,UAAM;AACJqC,MAAAA,MADI;AAEJ3B,MAAAA;AAFI,QAGFhC,uBAAuB,CAACyD,QAAD,EAAWvC,OAAO,CAACyC,MAAnB,EAA2BzC,OAAO,CAACc,MAAnC,CAH3B;;AAKA,QAAIA,MAAM,KAAK,CAAf,EAAkB;AAChB;AACD,KAd8C,CAgB/C;AACA;;;AACA,UAAMf,SAAS,GAAG,IAAIR,MAAJ,CAAW;AAC3BmD,MAAAA,WAAW,EAAE;AADc,KAAX,CAAlB;AAGA,UAAM/C,KAAK,GAAGR,QAAQ,EAAtB;AAEAY,IAAAA,SAAS,CAACgC,GAAV,CAAc,YAAY;AACxB,YAAMvC,OAAO,CAACC,UAAD,EAAaC,IAAb,EAAmBC,KAAnB,EAA0B,CAA1B,EAA6B8C,MAA7B,EAAqCA,MAAM,GAAG3B,MAA9C,EAAsDf,SAAtD,EAAiEC,OAAjE,CAAb;AACD,KAFD;AAIAD,IAAAA,SAAS,CAAC4C,EAAV,CAAa,OAAb,EAAsBC,KAAK,IAAI;AAC7BjD,MAAAA,KAAK,CAACG,GAAN,CAAU8C,KAAV;AACD,KAFD;AAIA,QAAIC,IAAI,GAAG,CAAX;;AAEA,eAAW,MAAMpC,GAAjB,IAAwBd,KAAxB,EAA+B;AAC7B,UAAIc,GAAG,IAAI,IAAX,EAAiB;AACf;AACD;;AAEDoC,MAAAA,IAAI,IAAIpC,GAAG,CAACC,UAAZ;;AAEA,UAAImC,IAAI,KAAK/B,MAAb,EAAqB;AACnBnB,QAAAA,KAAK,CAACG,GAAN;AACD;;AAED,YAAMW,GAAN;AACD;AACF;;AAED,SAAO6B,gBAAP;AACD,CArDD;;AAuDA,eAAeN,WAAf","sourcesContent":["import extractDataFromBlock from '../../../utils/extract-data-from-block.js'\nimport validateOffsetAndLength from '../../../utils/validate-offset-and-length.js'\nimport { UnixFS } from 'ipfs-unixfs'\nimport errCode from 'err-code'\nimport * as dagPb from '@ipld/dag-pb'\nimport * as raw from 'multiformats/codecs/raw'\nimport { pushable } from 'it-pushable'\nimport parallel from 'it-parallel'\nimport { pipe } from 'it-pipe'\nimport map from 'it-map'\nimport PQueue from 'p-queue'\n\n/**\n * @typedef {import('../../../types').ExporterOptions} ExporterOptions\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {Blockstore} blockstore\n * @param {PBNode | Uint8Array} node\n * @param {import('it-pushable').Pushable<Uint8Array>} queue\n * @param {number} streamPosition\n * @param {number} start\n * @param {number} end\n * @param {PQueue} walkQueue\n * @param {ExporterOptions} options\n * @returns {Promise<void>}\n */\nasync function walkDAG (blockstore, node, queue, streamPosition, start, end, walkQueue, options) {\n  // a `raw` node\n  if (node instanceof Uint8Array) {\n    queue.push(extractDataFromBlock(node, streamPosition, start, end))\n\n    return\n  }\n\n  if (node.Data == null) {\n    throw errCode(new Error('no data in PBNode'), 'ERR_NOT_UNIXFS')\n  }\n\n  /** @type {UnixFS} */\n  let file\n\n  try {\n    file = UnixFS.unmarshal(node.Data)\n  } catch (/** @type {any} */ err) {\n    throw errCode(err, 'ERR_NOT_UNIXFS')\n  }\n\n  // might be a unixfs `raw` node or have data on intermediate nodes\n  if (file.data != null) {\n    const data = file.data\n    const buf = extractDataFromBlock(data, streamPosition, start, end)\n\n    queue.push(buf)\n\n    streamPosition += buf.byteLength\n  }\n\n  /** @type {Array<{ link: PBLink, blockStart: number }>} */\n  const childOps = []\n\n  for (let i = 0; i < node.Links.length; i++) {\n    const childLink = node.Links[i]\n    const childStart = streamPosition // inclusive\n    const childEnd = childStart + file.blockSizes[i] // exclusive\n\n    if ((start >= childStart && start < childEnd) || // child has offset byte\n        (end >= childStart && end <= childEnd) || // child has end byte\n        (start < childStart && end > childEnd)) { // child is between offset and end bytes\n      childOps.push({\n        link: childLink,\n        blockStart: streamPosition\n      })\n    }\n\n    streamPosition = childEnd\n\n    if (streamPosition > end) {\n      break\n    }\n  }\n\n  await pipe(\n    childOps,\n    (source) => map(source, (op) => {\n      return async () => {\n        const block = await blockstore.get(op.link.Hash, {\n          signal: options.signal\n        })\n\n        return {\n          ...op,\n          block\n        }\n      }\n    }),\n    (source) => parallel(source, {\n      ordered: true\n    }),\n    async (source) => {\n      for await (const { link, block, blockStart } of source) {\n        /** @type {PBNode | Uint8Array} */\n        let child\n        switch (link.Hash.code) {\n          case dagPb.code:\n            child = dagPb.decode(block)\n            break\n          case raw.code:\n            child = block\n            break\n          default:\n            queue.end(errCode(new Error(`Unsupported codec: ${link.Hash.code}`), 'ERR_NOT_UNIXFS'))\n            return\n        }\n\n        walkQueue.add(async () => {\n          await walkDAG(blockstore, child, queue, blockStart, start, end, walkQueue, options)\n        })\n      }\n    }\n  )\n}\n\n/**\n * @type {import('../').UnixfsV1Resolver}\n */\nconst fileContent = (cid, node, unixfs, path, resolve, depth, blockstore) => {\n  /**\n   * @param {ExporterOptions} options\n   */\n  async function * yieldFileContent (options = {}) {\n    const fileSize = unixfs.fileSize()\n\n    if (fileSize === undefined) {\n      throw new Error('File was a directory')\n    }\n\n    const {\n      offset,\n      length\n    } = validateOffsetAndLength(fileSize, options.offset, options.length)\n\n    if (length === 0) {\n      return\n    }\n\n    // use a queue to walk the DAG instead of recursion to ensure very deep DAGs\n    // don't overflow the stack\n    const walkQueue = new PQueue({\n      concurrency: 1\n    })\n    const queue = pushable()\n\n    walkQueue.add(async () => {\n      await walkDAG(blockstore, node, queue, 0, offset, offset + length, walkQueue, options)\n    })\n\n    walkQueue.on('error', error => {\n      queue.end(error)\n    })\n\n    let read = 0\n\n    for await (const buf of queue) {\n      if (buf == null) {\n        continue\n      }\n\n      read += buf.byteLength\n\n      if (read === length) {\n        queue.end()\n      }\n\n      yield buf\n    }\n  }\n\n  return yieldFileContent\n}\n\nexport default fileContent\n"]},"metadata":{},"sourceType":"module"}